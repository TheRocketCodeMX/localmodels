# GPT-OSS con Ollama + Docker 

Esta es la versi√≥n **optimizada** usando Ollama para ejecutar GPT-OSS localmente con APIs estructuradas y compatibles con OpenAI.

## ¬øPor qu√© Ollama es mejor?

| Ventaja | Descripci√≥n |
|---------|-------------|
| **APIs REST nativas** | Compatible 100% con OpenAI API |
| **Menos memoria** | Modelos cuantizados autom√°ticamente |
| **Gesti√≥n simplificada** | Auto-descarga y gesti√≥n de modelos |
| **Mejor performance** | Optimizado para hardware consumer |
| **M√∫ltiples interfaces** | REST, OpenAI SDK, WebUI |

## Configuraci√≥n R√°pida

1. **Configura variables de entorno:**
   ```bash
   cp .env.example .env
   # Edita .env con tu token de ngrok
   ```

2. **Ejecuta con Docker Compose:**
   ```bash
   docker-compose up --build
   ```

## Servicios Incluidos

- **Ollama** (Puerto 11434): Motor de IA
- **API Gateway** (Puerto 8000): Tu API personalizada + ngrok
- **Web UI** (Puerto 3000): Interfaz web para testing

## Endpoints Disponibles

### 1. API Personalizada (Puerto 8000)
```bash
# Chat b√°sico
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Expl√≠came quantum computing",
    "max_tokens": 300,
    "temperature": 0.7,
    "reasoning": "high"
  }'

# Informaci√≥n del sistema
curl http://localhost:8000/

# Estado de salud
curl http://localhost:8000/health

# Listar modelos
curl http://localhost:8000/models
```

### 2. OpenAI Compatible (Puerto 8000)
```python
from openai import OpenAI

# Usa tu endpoint p√∫blico de ngrok o local
client = OpenAI(
    base_url="http://localhost:8000/v1",  # O tu URL de ngrok
    api_key="dummy"
)

response = client.chat.completions.create(
    model="gpt-oss:20b",
    messages=[
        {"role": "system", "content": "Eres un experto en IA."},
        {"role": "user", "content": "¬øQu√© es GPT-OSS?"}
    ],
    max_tokens=200,
    temperature=0.7
)

print(response.choices[0].message.content)
```

### 3. Ollama Directo (Puerto 11434)
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"
)

response = client.chat.completions.create(
    model="gpt-oss:20b",
    messages=[{"role": "user", "content": "Hola"}]
)
```

## Modelos Disponibles

- **gpt-oss:20b** - 21B par√°metros (~16GB RAM)
- **gpt-oss:120b** - 117B par√°metros (~80GB RAM)

Los modelos se descargan autom√°ticamente la primera vez.

## Comandos √ötiles

```bash
# Ver logs en tiempo real
docker-compose logs -f

# Solo Ollama sin gateway
docker run -d -v ollama:/root/.ollama -p 11434:11434 ollama/ollama

# Descargar modelo manualmente
docker exec -it <ollama_container> ollama pull gpt-oss:20b

# Listar modelos descargados
docker exec -it <ollama_container> ollama list
```

## Testing con Web UI

Accede a `http://localhost:3000` para una interfaz web completa donde puedes:
- Chatear con GPT-OSS
- Ajustar par√°metros
- Ver estad√≠sticas de uso
- Probar diferentes prompts

## Ventajas vs. Implementaci√≥n Directa

‚úÖ **APIs estructuradas y est√°ndar**  
‚úÖ **Compatible con cualquier cliente OpenAI**  
‚úÖ **Menor uso de memoria (cuantizaci√≥n autom√°tica)**  
‚úÖ **Gesti√≥n autom√°tica de modelos**  
‚úÖ **M√∫ltiples interfaces (REST, SDK, Web)**  
‚úÖ **Mejor optimizaci√≥n de hardware**  
‚úÖ **Streaming nativo**  
‚úÖ **M√©tricas y monitoring integrado**

## URLs Finales

Cuando ejecutes el stack completo tendr√°s:
- **API Local**: `http://localhost:8000`
- **Ollama Directo**: `http://localhost:11434`
- **Web UI**: `http://localhost:3000`
- **Ngrok P√∫blico**: Se muestra en los logs

¬°Perfecto para desarrollo y producci√≥n! üöÄ
